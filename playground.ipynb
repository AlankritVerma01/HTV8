{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-15T06:08:47.274628600Z",
     "start_time": "2023-10-15T06:08:47.267978300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, requests, json, openai, PyPDF2, pdfplumber, re, tiktoken, asyncio\n",
    "from bson.json_util import dumps\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "from io import BytesIO\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "pdf_file = \"cell.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# async def your_function(input_str):\n",
    "#     await asyncio.sleep(1)  # Simulating an async task\n",
    "#     return f\"Processed: {input_str}\"\n",
    "#\n",
    "# input_strings = [\"Input1\", \"Input2\", \"Input3\"]\n",
    "#\n",
    "# async def main():\n",
    "#     tasks = [your_function(input_str) for input_str in input_strings]\n",
    "#     results = await asyncio.gather(*tasks)\n",
    "#\n",
    "#     for input_str, result in zip(input_strings, results):\n",
    "#         print(f\"Input: {input_str} - Output: {result}\")\n",
    "#\n",
    "#\n",
    "# asyncio.run(main())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def user_said(content, history):\n",
    "    history.append({\"role\":\"user\", \"content\":content})\n",
    "\n",
    "def assistant_said(content, history):\n",
    "    history.append({\"role\":\"assistant\", \"content\":content})\n",
    "\n",
    "def ask_chatgpt(user, history, system=None, new_chat=False, max_tokens=256, only_response=False, temp=0, model='gpt-3.5-turbo'):\n",
    "\n",
    "    history = [] if new_chat else history\n",
    "\n",
    "    if system and new_chat:\n",
    "        history.append({\"role\":\"system\", \"content\":system})\n",
    "    user_said(user, history)\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=model,\n",
    "      messages=history,\n",
    "      temperature=temp,\n",
    "      max_tokens=max_tokens,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    response = response['choices'][0]['message']['content']\n",
    "\n",
    "    if only_response:\n",
    "        return response\n",
    "    else:\n",
    "        assistant_said(response, history)\n",
    "        return response, history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "class SectionNode:\n",
    "\n",
    "    def __init__(self, title, page, top):\n",
    "        self.id = str(id(self))\n",
    "        self.title = title\n",
    "        self.text = \"\"\n",
    "        self.children = []\n",
    "        self.page = page\n",
    "        self.top = top\n",
    "        self.depth = None\n",
    "\n",
    "    def addChild(self, other):\n",
    "        self.children.append(other)\n",
    "\n",
    "    def getSummary(self):\n",
    "        #TODO Use GPT to get the summary of text\n",
    "        self.text = self.title\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Depth: {self.depth}\\n' \\\n",
    "               f'Title: {self.title}\\n' \\\n",
    "               f'Page: {self.page}\\n' \\\n",
    "               f'Text: {self.text}\\n' \\\n",
    "               f'Children: {[child.title for child in self.children]}\\n'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T06:08:50.763833700Z",
     "start_time": "2023-10-15T06:08:50.756827400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "{'2334560093968': {'depth': 0,\n  'page': 0,\n  'top': None,\n  'title': 'Cell2Sentence: Teaching Large Language Models the Language of Biology',\n  'text': '',\n  'children': ['2334586778512',\n   '2334567165392',\n   '2334567165776',\n   '2334567166800']},\n '2334586778512': {'depth': 1,\n  'page': 1,\n  'top': 720,\n  'title': 'Introduction',\n  'text': 'Large language models (LLMs) such as GPT have demonstrated powerful capabilities in natural language processing tasks including question answering, summarization, and text generation ([1], [2], [3], [4], [5], [6]). However, applying LLMs to other domains like biology remains an open challenge. In particular, a method to directly apply existing LLMs to single-cell transcriptomics could enable new ways of analyzing, interpreting, and generating single-cell RNA sequencing data. Current methods in this domain rely on specialized neural networks that do not leverage the pretrained knowledge and language understanding of large language models. In this work, we aim to extend the capabilities of LLMs to the domain of transcriptomics. Our key idea is representing single-cell data in a text format amenable to language models through a method called Cell2Sentence (C2S) [7]. C2S transforms each cell’s gene expression profile into a plaintext sequence of gene names (also known as gene symbols) ordered by expression level (Figures 1 and 2). Importantly, we show this rank transformation can be reverted with minimal loss of information (Figures 3 and 7). This allows any pretrained causal language model to be further fine-tuned on cell sequences. Critically, we find that natural language pretraining followed by C2S training significantly improves model performance on transcriptomic tasks compared to training with C2S only, with performance additionally scaling with model size. Our fine-tuned models can generate cells by completing sequences of gene names, generate cells from natural language text prompts, and generate natural language text about cells. By leveraging LLMs’ vast pretrained knowledge and combining both modalities, we can enable models that not only generate and interpret transcriptomics data, but also interact in natural language. Potential applications include inferring how gene expression would change under perturbations, generating rare cell types, identifying gene markers, and interpreting transcriptomics via natural language. Such capabilities could aid biologists and advance single-cell research. To accomplish this, we first convert single-cell data into C2S format. We then fine-tune pretrained LLMs on these cell sequences using a simple and scalable method. We demonstrate that fine-tuned models can generate accurate cell types and understand transcriptomics data well enough to predict cell labels when prompted. Additional metadata can be provided for conditioning. We aim to keep our approach simple and as close to standard training pipelines in language modeling as possible. The benefits of adhering to this philosophy are twofold: 1.Easy to use. Any user can readily leverage any available pretrained language model by using popular third-party libraries like Hugging Face Transformers [8] that have streamlined model deployment and fine-tuning. The only additional steps needed are to preprocess raw single-cell data into C2S format, and to postprocess generated texts back into gene expression vectors, both done seamlessly using our modular data transformation pipeline provided in our GitHub repository. 2.Easy to modify. The simplicity of the training pipeline means the user can mold the preprocessing and training steps to their liking, e.g. by adding more metadata or prepending specific prompts. In summary, our key contributions are: 1.Introducing Cell2Sentence, an effective method for representing single-cell data as text sequences. 2.Showing large language models can be fine-tuned on cell sentences to generate cells and classify cell types. 3.Providing a simple and modular framework for adapting LLMs to transcriptomics using popular LLM libraries. In the following manuscript, we first explain the C2S data transformation and model fine-tuning process. We then evaluate fine-tuned LLMs on tasks including cell generation, classification, and expression recovery to demonstrate their biological understanding. Finally, we discuss the implica- tions and future directions for combining natural language and transcriptomics through representation learning. 2Figure 1: Overview of the Cell2Sentence framework. Input single-cell data, including metadata, are converted into cell sentences for LLM fine-tuning. Inference, via prompting, generates new cell sentences that can be converted back to gene expression space.',\n  'children': ['2334575306256']},\n '2334575306256': {'depth': 2,\n  'page': 2,\n  'top': 576.599,\n  'title': 'Related Work',\n  'text': 'Large language models Large language models have been adapted for a wide range of tasks on textual data in the field of natural language processing. Some key tasks and sample architectures include text classification (LSTM [9], BERT [3], RoBERTa [10]), question answering (LlaMA-2 [5], Falcon [11]), and text generation (T5 [12], GPT-3 [13], BART [14]). For a more in-depth discussion of developments around large language models, the reader is directed to [15]. Single-cell foundation models Alongside the development of large models for natural language processing, deep neural networks have been developed to accomplish numerous tasks on single- cell transcriptomics data. Architectures have been described for several tasks inluding: cellular annotation—where a cell is assigned a label according to it’s biological identity (NeuCA [16], ACTINN [16], scVI [17])—, batch effect removal/sample integration—where transcript abundance differences due to technical replicates are removed (scVI, scGen [18], SAUCIE [19])—, and data imputation—where missing transcript abundance data are inferred (scVI, DeepImpute [20], SAUCIE). More recently, efforts such as the Gene Expression Omnibus (GEO) [21] and Human Cell Atlas (HCA) [22] have centralized and standardized the storage of data from hundreds of single-cell experiments across a wide range of tissues, comprising hundreds of millions of measurements. Several models have been designed and trained on this data (e.g. scGPT [23], scFoundation [24], Geneformer [25]), with the goal of creating a foundation model for single cell transcriptomics data analogous to foundation models in natural language processing. Prompt fine-tuning Since the introduction of GPT-2 [26], prompting has become a common method to elicit meaningful behavior from large language models ([27], [28], [29]). Recently, publicly available datasets like Alpaca [30] and dataset generators such as Flan [31], [32], [33] together with parameter efficient fine-tuning [34] have made it possible to train custom large language models. A survey of methods can be found in [35]. Multimodal training and cross-modality encoding Unlike most multimodal machine learning schemes, which perform encoding of each modality separately either to a shared latent space, or parallel latent spaces coordinated by a constraint, our C2S framework transforms data directly into a single format (text) prior to embedding [36, 37]. One previously described analogous approach is the visual language modeling strategy of [38], which transformed images into a visual bag-of-words format prior to classification. However, to our knowledge, no such analogous approaches have been described using modern embedding strategies and in the domain of single-cell transcriptomics.',\n  'children': []},\n '2334567165392': {'depth': 1,\n  'page': 2,\n  'top': 148.948,\n  'title': 'Results',\n  'text': 'In this section, we present several benchmarks and evaluations to demonstrate the different use cases of Cell2Sentence. All presented models are trained on a human immune tissue dataset [39] (see Section 3). \"NL + C2S\" models are pretrained on natural language and then fine-tuned on cell sentences. \"C2S\" models are only trained on cell sentences. 3Figure 2: Detailed overview of the Cell2Sentence framework. Single-cell gene expression profiles are transformed into cell sentences via expression rank orderering of gene names. Cell sentences may be annotated with biological metadata, such as cell type, tissue, or disease. LLMs are then fine-tuned on the cell sentences. Inference is done by generating cells via autoregressive cell completion, generating cells from text, or generating text from cells. The resulting generated cell sentences can be converted back to gene expression.',\n  'children': ['2334567165520', '2334567165648']},\n '2334567165520': {'depth': 2,\n  'page': 3,\n  'top': 421.406,\n  'title': 'Cell sentence encoding is a robust and reversible operation',\n  'text': 'Gene rank and gene expression follow a log-linear relationship in scRNAseq data. Single-cell RNA sequencing produces transcript count matrices that represent the genetic profiles of individual cells. Most current computational models in single-cell biology concentrate on handling data in Rc×n, posing scalability challenges with larger datasets. We propose transforming expression matrices into gene sequences as a solution to enable the use of LLMs [40, 41] and other transformer-based architectures [42] for single-cell data analysis. While genes are not intrinsically ordered in transcript matrices, their expression patterns have been shown to follow inverse-rank frequency patterns [43, 44], thus establishing a steady relationship between a gene’s expression level within a cell and its rank among the genes expressed in that cell. We model this inverse-rank relationship with a log-linear distribution and approximate it in log-log space using a linear regression [7]. The resulting models allow us to convert cells back and forth between gene rank and expression domains (see Section 3.4). We leverage this capability to produce rank-ordered sequences of gene names which we use to train our subsequent language models (see Section 3.1). We present visualizations for the relationship between normalized gene expression and rank on a variety of human tissue datasets in Appendix Figure 7, as well as additional metrics on the performance of gene expression reconstruction in Appendix Figure 8. C2S enables forward and reverse transformation with minimal information loss. We first evaluate the effectiveness of our C2S transformation to reconstruct the original gene expression of a cell from cell sentence format. Figure 3A visualizes the reconstruction performance of a fitted linear regression on an immune tissue dataset comprising 50K cells and over 35K genes. A simple linear model captures over 81% of the variation in the gene expression, requiring only the log rank value of that gene for expression reconstruction. This demonstrates that the transformation to cell sentences and back to expression space preserves much of the important information in single-cell data. This allows for analysis in the natural language space of cell sentences followed by accurate conversion back to expression. Figures 3B and 3C visualize the original ground truth immune tissue data alongside reconstructed expression data from converted cell sentences. This qualitatively shows that important structure in the immune tissue data as well as cell type separation are retained. 4Figure 3: Cell2Sentence transformation can accurately reconstruct cell expression from cell sentences. (A) Expression reconstruction from rank using a linear model. (B) UMAP of ground truth expression (top) versus reconstructed expression from cell sentences (bottom). (C) UMAP of ground truth expression (green) and reconstructed expression from cell sentences overlaid. Model Pearson R Spearman R R2 GPT-2 Small (C2S) 0.947 0.766 0.873 GPT-2 Small (NL + C2S) 0.984 0.768 0.949 GPT-2 Medium (C2S) 0.948 0.781 0.88 GPT-2 Medium (NL + C2S) 0.982 0.781 0.943 Table 1: Correlation metrics for averaged generated cells per cell type against original expression values. \"NL + C2S\" means pretrained on natural language and then trained on cell sentences. \"C2S\" means no pretraining and just trained on cell sentences. Correlation metrics are computed per cell type, and are averaged across all 17 cell types, forming a score for the quality of an average generated cell.',\n  'children': []},\n '2334567165648': {'depth': 2,\n  'page': 4,\n  'top': 258.575,\n  'title': 'LLMs can meaningfully manipulate cells as text',\n  'text': 'LLMs trained on cell sentences show healthy convergence behavior. We find that infusing sequential structure to single-cell data yields a non-trivial and meaningful textual modality. We train four GPT-2 models [26] on a corpus of cell sentences sampled from [39]. While we pretrain models exclusively on cell sentences, we also experiment with fine-tuning pretrained GPT-2 models. We find that all our models learn cell sentence distributions and converge during training with a steadily decreasing perplexity (see Figure 5). In both settings, we find that larger models achieve lower perplexities. Overall, we demonstrate the capability of causal language models to learn the cell sentence semantic distribution from a narrow scRNA-seq dataset. Cell reconstruction: R2gene prediction. Accurate generation of different cell types is crucial for generative approaches on single-cell data, as it enables downstream analysis using the model. To evaluate the ability of our trained models to generate realistic cells, we consider the average generated cell of each of the 17 cell types in our immune tissue dataset, and compare it with the average real cell of each cell type in Table 1. Across 17 different cell types, generated cells from finetuned models show high correlation with real cells, capturing over 94% of the variation in the expression of an 5average cell. We note that initializing a model with a pretrained language model outperforms training from scratch, indicating that there is mutual information which allows a model to better understand cell sentence generation. Generate cells from text: generate cells from cell type label. We assess the model’s performance by calculating the k-Nearest Neighbors (KNN) accuracy for generated cells using two distinct methods: 1.Classify the generated cell type based on the types of its nearest neighbors in the ground-truth dataset. 2. Classify the generated cell type based on the types of its generated neighbors. The label assigned to a generated cell corresponds to the cell type used for its conditional generation, as detailed in Section 3.2. Predictions of type 1 determine if the model is capable of approximating real cells within the corresponding cell type, whereas predictions of type 2 show the model is capable of generating distinct cell types. The UMAP plots in Figure 4 show how much separation can be achieved by representing cells with the 100 highest expressed genes. Unlike reducing the size of the gene expression matrix by selecting a subset of highly variable genes, our approach allows different cells to be encoded with the specific genes that are most relevant to them, potentially allowing for better representation of rare cell types at similar levels of compression. Additionally, since the immune dataset has many sub-types for some cell types (e.g. T-cells), we expect some sub-types to be very close when restricted to the 100 genes with the highest expression. Still, nontrivial structure emerges in all of the plots. In Figure 4C and D, the UMAP plots demonstrate that the reconstructed gene expression from the top 100 generated genes not only maintains the general structure of the original data but also closely aligns with cell type-specific distinctions in the baseline UMAP visualizations. This validates our generative model’s ability to capture both macroscopic and fine-grained expression profiles, attesting to its efficacy in creating biologically relevant cellular representations. To further corroborate the gene expression patterns of our generated cells, we employ gene expression heatmaps, presented in Figure 10. These heatmaps compare the differentially expressed genes for each cell type in both the generated and ground-truth cells, with the differential genes sourced from the top 100 genes in the ground truth dataset. The heatmaps confirm that the generated data is consistent with the overall structure in the original dataset while also highlighting subtle gene-level discrepancies. This enhances our understanding of how well the generated cells mimic the intricacies of the baseline data. Additionally, the analysis points to promising avenues for using generated cells to identify key marker genes, offering critical insights into cellular function and laying the groundwork for future research. ModelK=5 K=10 K=25 K=50 Expr . Lev. Expr. Lev. Expr. Lev. Expr. Lev. Real cells (ground truth): 62.60 37.55 62.32 39.86 60.45 42.64 58.31 44.33 Generated cells: scVI 43.06 - 44.24 - 45.06 - 43.63 - GPT -2 Small (C2S) 24.56 16.02 23.79 17.72 23.51 19.17 22.84 19.76 GPT -2 Small (NL + C2S) 52.55 37.63 52.19 41.20 51.42 43.42 49.51 44.44 GPT -2 Medium (C2S) 26.36 17.58 25.48 19.18 24.55 20.67 23.44 21.32 GPT -2 Medium (NL + C2S) 54.67 38.60 54.52 41.34 53.27 43.93 51.80 44.73 Table 2: KNN classification accuracy results against ground truth data. \"NL + C2S\" means pretrained on natural language and then trained on cell sentences. \"C2S\" means no pretraining and just trained on cell sentences. KNN classifier is fitted on ground truth cell sentences and used to predict the cell type label of generated cell sentences from different trained models. KNN classification is done both in cell sentence space using Levenshtein distance (Lev.), as well as after converting back to expression vectors (Expr.). \"Real cells\" indicates KNN classification fit on ground truth cell sentences and used to predict a separate sample of ground truth cell sentences. Tables 2 and 3 shows the KNN accuracy of each model for different number of neighbors. Generated cells from our trained models are accurately classified by cell type by a KNN classifier 2, with up 6. CC-BY-NC-ND 4.0 International license available under a(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made The copyright holder for this preprint this version posted September 14, 2023. ; https://doi.org/10.1101/2023.09.11.557287doi: bioRxiv preprint ModelK=5 K=10 K=25 K=50 Expr . Lev. Expr. Lev. Expr. Lev. Expr. Lev. scVI 45.72 - 47.04 - 45.36 - 39.90 - GPT -2 Small (C2S) 28.65 70.61 30.03 52.26 31.68 40.88 32.66 36.21 GPT -2 Small (NL + C2S) 60.07 76.09 60.41 66.95 60.55 60.04 59.42 56.80 GPT -2 Medium (C2S) 32.28 69.57 34.00 54.50 34.57 43.93 33.78 39.71 GPT -2 Medium (NL + C2S) 62.95 76.67 62.98 67.77 63.09 60.95 62.05 57.51 Table 3: KNN classification results for separation of distinct cell types. \"NL + C2S\" means pretrained on natural language and then trained on cell sentences. \"C2S\" means no pretraining and just trained on cell sentences. This measures the model’s ability to generate distinct clusters when conditioned on cell type. This is done both in cell sentence space using Levenshtein distance (Lev.), as well as after converting back into expression vectors (Expr.). to 54% classification accuracy. The distinctness of generated cells is further quantified in Table 3, showing that our models are capable of generating distinct cell types through natural language. We compare against scVI [45], a probabilistic generative model for single-cell data based on variational inference. We observe that our models outperform scVI in terms of generation quality against the ground truth cells. Additional UMAPs of generated cells from scVI are available in Figures 13 and 14. We require cell types to have at least 500 samples in the training dataset so the number of training samples per cell type is large enough to be meaningful for comparison. This filtering condition should be dropped for larger training datasets. See Sections 2.2 and 3.4 for details on transforming cell sentences to expression values. Generate text from cells: autoregressive cell type prediction. Models pretrained with natural language (NL) can be trained with Cell2Sentence to generate meaningful text from cells. Our results illustrate the efficacy of this approach through autoregressive prediction of cell types. Note this is distinct from traditional classification—we do not train a classifier head or modify the architecture in any way. We simply prompt the model with a cell sentence and ask it to identify its cell type in natural language (see Figure 6). Table 4 underscores the necessity of NL pretraining for accurate cell type identification. Figure 12 presents the corresponding confusion matrices. A significant performance decline is observed when using models that have not undergone NL pretraining, thereby confirming that the models are not merely memorizing the conditioning text. Despite 1) the limited scope of natural language text in our training prompts relative to the pretraining corpus, and 2) permitting the models to train on these natural language prompts, models without NL pretraining failed to acquire meaningful natural language embeddings. Furthermore, a modest performance increment is observed as the scale of pretrained models increases. Model Accuracy F1 Precision Recall Random Classifier 2.86 2.69 2.86 2.57 GPT-2 Small (C2S) 29.33 17.46 13.27 29.33 GPT-2 Small (NL + C2S) 69.95 68.56 69.44 69.95 GPT-2 Medium (C2S) 29.17 17.41 13.35 19.17 GPT-2 Medium (NL + C2S) 74.26 73.69 74.16 74.26 Table 4: Quantification of autoregressive cell type prediction on unseen cells. \"NL + C2S\" means pretrained on natural language and then trained on cell sentences. \"C2S\" means no pretraining and just trained on cell sentences. These results show test accuracy significantly improves with NL pretraining. The scores are computed on unseen immune tissue test data and weighted by the distribution of labels. 7. CC-BY-NC-ND 4.0 International license available under a(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made The copyright holder for this preprint this version posted September 14, 2023. ; https://doi.org/10.1101/2023.09.11.557287doi: bioRxiv preprint Figure 4: Plots for cell sentences and expression data. The UMAP plots show that models pretrained with language and fine-tuned with Cell2Sentence achieve quality generated outputs in the sentence space. We see good cell type separation for both generated and ground truth cell sentences in addition to good overlap, showing that our generated sentences model the ground truth distribution well. Plots A, B, and C are UMAP plots of generated cell sentences versus real cell sentences using Levenshtein distance. We computed the Maximum Mean Discrepancy (MMD) [46] statistic with the Python library scMMD [47]. Plots D, E, and F are UMAP plots of generated cell expression vectors and ground truth cell expression vectors. See Table 2 and Table 3 for a quantification of these plots.',\n  'children': []},\n '2334567165776': {'depth': 1,\n  'page': 7,\n  'top': 216.711,\n  'title': 'Methods',\n  'text': '3 Methods',\n  'children': ['2334567165904',\n   '2334567166032',\n   '2334567166544',\n   '2334567166672']},\n '2334567165904': {'depth': 2,\n  'page': 7,\n  'top': 197.248,\n  'title': 'Data transformation',\n  'text': 'The Cell2Sentence transformation is at its core a reorganization of the cell expression matrix into sequences of gene names ordered by decreasing transcript abundance, similar to rank-ordering transformations of count matrices [44]. Let Cdenote a cell by gene count matrix with nrows and k genes, with Ci,jdenoting the number of RNA molecules observed for gene jin cell i. We follow standard preprocessing steps for single-cell RNA seq data, including filtering cells with fewer than 200 genes expressed and filtering genes which are expressed in fewer than 200 cells. Quality control metrics are then calculated based on mitochondrial gene counts within each cell using the Scanpy Python library [48], and low-quality cells are filtered out which contain over 2500 counts, or which have greater than 20 percent of transcript counts from mitochondrial genes. The count matrix is then 8row-normalized so that each cell sums up to 10,000 transcript counts and then log-normalized [49], obtaining the final preprocessed count matrix C′. We summarize this normalization step as: C′ i,j= log10  1 + 104×Ci,jPk j=1Ci,k! (1) We denote the rank-order transformation applied on C′asS, and the sequence of gene names resulting fromS(Ci)as cell sentence sifor each cell iin the preprocessed count matrix. In practice, we apply the preprocessing and rank-order transformation Son each individual single-cell dataset, providing a flexible process for converting traditional single-cell gene expression count matrices to cell sentences. We construct our dataset by sampling 49,920 cells from a large dataset of human immune tissue cells [39]. We apply the previously outlined normalization steps and convert to cell sentences. We split the resulting cell sentences into training (39,936), validation (4,992), and test (4,992) sets. We also attach each cell’s type as specified in [39]. In order to limit computational costs, we truncate each cell sentence to only keep the 100 highest expressed (top 100 ranked) genes. Note that this truncation operation minimizes the resulting ordering variability, as genes with lower rank have more similar expression values. We consider this as a future point of study.',\n  'children': []},\n '2334567166032': {'depth': 2,\n  'page': 8,\n  'top': 514.996,\n  'title': 'Training',\n  'text': 'We trained a total of four GPT-2 models for this study. Specifically, we pretrain and fine-tune both GPT-2 small and medium on our cell sentence corpus. In both settings, we format inputs to the model as prompts, providing natural language context for the model to learn from (Figures 1 and 2). We follow a standard training configuration and use the AdamW optimizer [50]. We leverage half- precision floating points (FP16) and gradient accumulation for memory savings. We found negligible improvement with using full-precision floating points (FP32) at the cost of a 60% slowdown.',\n  'children': ['2334567166160', '2334567166288', '2334567166416']},\n '2334567166160': {'depth': 3,\n  'page': 8,\n  'top': 417.549,\n  'title': 'Tasks',\n  'text': 'As outlined in Section 2, we formulate three downstream tasks (see Figure 6): 1.Unconditional cell sentence generation: produce a sequence of 100 genes without any prescribed cell type label. 2.Conditional cell sentence generation: generate a sequence of 100 genes given a specific cell type label. 3.Cell type prediction: generate a cell type label following a provided sequence of 100 genes. During the training phase, for each iteration, we randomly select a task and subsequently pick a corresponding prompt template from a set of approximately 20 templates per task. These templates, while varied in phrasing, retain consistent semantic meaning. The prompt structure for cell type prediction combines the prompt with the cell sentence. For conditional cell generation, it merges the prompt with the specified cell type. In contrast, the unconditional cell generation prompt primarily consists of a succinct directive, as depicted in Figure 6.',\n  'children': []},\n '2334567166288': {'depth': 3,\n  'page': 8,\n  'top': 230.341,\n  'title': 'Pretraining',\n  'text': 'The GPT-2 small model is initialized with 12 layers and 768 hidden dimensions, and the medium model with 24 layers and 1024 hidden dimensions, as detailed in [26]. We employ a learning rate of6×10−4with a cosine scheduler and 1% warmup ratio. For the GPT-2 medium model, we accumulate gradients over 16 steps. The effective batch sizes for the small and medium models are of 10 and 48 examples. Each model is trained using a single A5000 GPU over two days. We train a Byte Pair Encoding (BPE) tokenizer [51] on the full cell sentence dataset, including NL prompts and cell type labels, yielding a vocabulary of 9,609 tokens. The training set contains approximately 30 million tokens, averaging 740 tokens per example. Due to the smaller embedding space, the initialized models contain slightly fewer parameters than their counterparts pretrained on a vocabulary of 50,257 tokens (93M for the small model and 313M for medium model, as shown in Figure 5). The resulting corpus exhibits sparse NL tokens due to short and repetitive prompts. Despite instruction corpora being traditionally used to fine-tune pretrained models for question 9Figure 5: Perplexity curves for finetuned and pretrained models. (A) Estimated model perplexity computed after the training loss. (B) Estimated model perplexity computed on the validation set during training. All models preserve the default GPT-2 context length of 1024 tokens. answering tasks, we adopt this setting during pretraining to mirror our fine-tuning setup described in Section 3.2.3. We hypothesize that the semantic variability from prompting patterns might implicitly regularize token and positional embeddings, with natural language tokens acting as class tokens. We emphasize that the loss is computed on both the prompt and the associated label (i.e. cell type). Not doing so would cause embeddings of the prompt tokens to remain random, impairing the capacity of the model to learn the conditional relations between prompt and label tokens. We evaluate the capacity of our model to generate valid genes and maintain an accurate sequence length (here, of 100 genes) and present the results in Table 5. We find that both pretrained models are able to generate sequences of 100 genes without significantly deviating from the mean. The models also both achieve over 97% and 96% accuracy in gene validity and uniqueness.',\n  'children': []},\n '2334567166416': {'depth': 3,\n  'page': 9,\n  'top': 359.439,\n  'title': 'Fine-tuning',\n  'text': 'Both models are initialized using pretrained weights from the Hugging Face model hub [52]. We employ a learning rate of 5×10−5with a linear scheduler. On both models, we accumulate gradients over 16 steps and use batch sizes of eight examples (yielding an effective gradient update batch size of 128 examples). Each model is trained using a single A5000 GPU. While we experimented with applying efficient fine-tuning techniques (e.g. LoRA [34]), fully fine-tuned models outperformed alternatives in gene uniqueness and validity assessments. We notably found LoRA to yield highly variable generation patterns, with uniqueness of genes in generated sentences as low as 70%. Unlike for our pretraining setup, we apply the instruction fine-tuning task in a classical manner, computing the loss exclusively on labels. We use the pretrained GPT-2 tokenizer, which averages around 233 tokens per training samples (yielding a total of 9M training tokens). Similarly to Section 3.2.2, we examine the coherence of generated output using sequence length, as well as accuracy in gene validity and uniqueness. We find that the fine-tuned model outperform the pretrained models by generating genes with over 99% validity and 98% uniqueness on average. While both models achieve reliable performance by these standard metrics, we conclude that our fine-tuned models are consistently outperforming the pretrained models in generating real human genes, which are only rarely duplicated within cell sentences.',\n  'children': []},\n '2334567166544': {'depth': 2,\n  'page': 9,\n  'top': 140.779,\n  'title': 'Inference',\n  'text': 'At inference, we follow the training procedure in Section 3.2 (see Figures 1, 2, and 6). We set the hyperparameters top_p = 0.95 and top_k = 50 to promote diversity in cell generation. For conditional cell generation and unconditional cell generation used in our experiments from Section 2.2, we randomly sample prompt templates as input, inserting the cell type where needed for 10Model Gen. Length Valid Genes % Unique Genes % GPT-2 Small (C2S) 101.54 97.84 97.31 GPT-2 Small (NL + C2S) 99.84 99.60 98.88 GPT-2 Medium (C2S) 100.62 97.51 96.69 GPT-2 Medium (NL + C2S) 99.80 99.70 99.47 Table 5: Quality of generated outputs. \"NL + C2S\" means pretrained on natural language and then trained on cell sentences. \"C2S\" means no pretraining and just trained on cell sentences. This table shows 1. models trained using Cell2Sentence are able to generate real genes with few duplicates and nonsense genes and 2. models pretrained with natural language generate more accurately. The metrics are computed across all 35 cell types seen during training with 500 cells generated per cell type and then averaged across all generated cells (top 100 genes). The valid genes percentage shows the number of genes generated that are real genes including duplicates. The generated length is the number of genes generated regardless of their validity. The unique gene ratio is the ratio of unique valid genes to the generated length. Figure 6: Depiction of three types of prompts used during training and generation. From left to right: unconditional cell generation, conditional cell generation (e.g.: with cell type), and autoregressive cell type prediction. For our setup, we generate and prompt with 100 genes. conditional generation. For autoregressive cell type prediction, we randomly sample templates as in training but use either real cell sentences from our test dataset or generated cell sentences. All outputs are generated until an end-of-sequence (EOS) token is predicted, which was appended to all training samples. We have experimented with using non-special token delimiters such as semicolons and periods but found them unnecessary for our study’s scope. Post-generation, gene and cell type extraction is done using regex to remove prompts. For evaluation, we retain invalid genes and average ranks of duplicate genes, rearranging sequences as needed. When reverting back to expression values, invalid genes are ignored, but the rank values are preserved, e.g. if an invalid gene appears in position 3 and a valid gene appears in position 4, the invalid gene is ignored, but the valid gene retains a rank of 4. Utilizing pretrained LLMs offers the advantage of using highly optimized, open-source libraries for inference. We make use of flash attention [53, 54] and batched inference to accelerate generation. Inference for GPT-2 medium (345M parameters) can be done on a single A5000 GPU with 24GB of VRAM and a batch size of 100 without running out of memory. For GPT-2 small (117M parameters), the batch size can be increased to 250. We did not determine the exact maximum batch sizes, so these values can likely be increased further. On average, the number of tokens in the prompts and top 100 genes combined was around 350. For example, the GPT-2 small model takes approximate 20 minutes to generate 500 cells from each of the 35 cell types found in the immune tissue dataset. Model quantization was not required but may be useful for future experiments with larger models.',\n  'children': []},\n '2334567166672': {'depth': 2,\n  'page': 10,\n  'top': 151.411,\n  'title': 'Transformation back to expression',\n  'text': 'To transform generated cell sentences back to expression space, we define a simple inverse transfor- mation function using a linear model to predict the expression of the generated gene based on its rank. For a given single-cell dataset which underwent rank-order transformation S, letridenote the log of the rank of gene iinC′, andeithe original expression of gene i. We first fit a linear model to predict eifromriduring the initial conversion to cell sentence format, resulting in a fitted slope and 11intercept value which are saved for each converted dataset (see Figures 7, 8, and 9). Hence, we fit a linear regression of the form ei=ad×ri+bi, given dataset dand{ad, bd} ∈R2. Generated cell sentences output by different models are first run through a postprocessing procedure which replaces any invalid generated gene names with a special ignore token, and averages the rank of any duplicate genes in the generated cell sentence. The fitted linear model parameters are then applied to the log of the rank of the generated genes to convert the sequence of genes back to an expression vector. Note that any genes which are not present in the generated cell sentence are considered to have zero expression, and are filled with zeros in the resulting expression vector. We define the average rank of a generated gene ggen ibelonging to the set of unique genes GU⊆Sas follows: rgen i=1 |G||G|X j=1rank(ggen j) (2) where G={ggen 1, ggen 2, . . . , ggen n} ⊆Sis the set of duplicate generated genes for ggen i, and rgen i denotes the average rank of gene ggen iin the generated cell sentence. This yields the following formulation for expression value vector for the generated cell egen i=\\x1aad×log(rgen i) +bdifggen i∈G 0 otherwise(3) In practice, we consider a global dictionary of all gene names seen in single-cell datasets, which dictates the size of the resulting gene expression vector of the cell.',\n  'children': []},\n '2334567166800': {'depth': 1,\n  'page': 11,\n  'top': 466.738,\n  'title': 'Discussion',\n  'text': 'In this work, we presented Cell2Sentence, a new approach for facilitating the training of large language models on single-cell transcriptomics data. The method represents gene expression profiles in single cells as text sequences, which we term \"cell sentences\". These cell sentences are composed of gene names sorted by their expression levels, thereby creating a robust and reversible encoding of the biological data. Our investigations revealed that cell sentences conveniently and correctly encode gene expression data in a format easily digestible by existing language models. Language models fine-tuned on these cell sentences not only converge robustly, but also perform significantly better on tasks related to cell sentences as compared to models trained from scratch or other state-of-the-art deep learning models purpose-built for handling single-cell RNA sequencing data. Cell sentences can be seamlessly integrated with textual annotation to perform both generation and summarization tasks, both of which benefit from natural language pretraining. In fact, there are no theoretical limitations on the application of anytext-based architecture to Cell2Sentence-generated cell sentences. Our findings highlight the benefits of transfer learning in this interdisciplinary setting. While our research showcases the potential of leveraging language models for transcriptomics, there are several limitations that need to be acknowledged. Cell2Sentence captures gene expression data through a set of gene names sorted by their expression levels. This approach results in the loss of specific quantitative information about the levels of gene expression, requiring the model to infer the strength of expression from gene order alone. While our experiments suggest that the transformation between gene order and transcript counts is robust, there may be more information loss in certain conditions, such as experiments with a very high depth of sequencing and a large number of transcripts recovered per gene. It is worth noting that the natural language output from models fine-tuned using Cell2Sentence cannot be used directly without specific post-processing (e.g. to ensure that generated gene names are meaningful, and that genes are not duplicated in a sentence), though we have shown that, at least in our evaluations, such invalid output is exceedingly rare. Cell sentences have different characteristics and grammar than natural language, and model archi- tectures that have been designed to perform well on natural language such as GPT-2 may struggle with certain aspects of the structure of cell sentences. Unlike natural language sentences, which are typically composed of tens of individual words, cell sentences are composed of thousands of gene names. As resource requirements for most transformer-based architectures scale with the number of tokens processed in parallel, cell sentences in their original form may be computationallylengths. We believe that processing of cell sentences will benefit greatly from ongoing work in long-sequence/long-context language processing. Finally, our experiments, while exciting, focus only on a highly restricted set of cell types and tasks. Extending our method to more complex and heterogeneous datasets, as well as to additional tasks enabled by the direct application of existing natural language model architectures is critical to validate the versatility and reliability of Cell2Sentence. In summary, our study serves as a foundational effort that opens the door for the integration of language models with other computational biology techniques. ',\n  'children': []}}"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_graph(pdf_reader, title, page):\n",
    "    root = SectionNode(title, page, None)\n",
    "    get_bookmark(root, pdf_reader.outline)\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "def get_bookmark(root, outline):\n",
    "    new = None\n",
    "\n",
    "    for item in outline:\n",
    "        if isinstance(item, list):\n",
    "            get_bookmark(new, item)\n",
    "        else:\n",
    "            new = SectionNode(item.title, item.page, item.top)\n",
    "            root.addChild(new)\n",
    "\n",
    "\n",
    "# def get_json_output(json_output, root):\n",
    "#     json_output.append(root.getJson())\n",
    "#     for child in root.children:\n",
    "#         get_json_output(json_output, child)\n",
    "\n",
    "\n",
    "def get_json_output(json_output, root):\n",
    "    json_output[root.id] = {}\n",
    "    json_output[root.id][\"depth\"] = root.depth\n",
    "    json_output[root.id][\"page\"] = root.page\n",
    "    json_output[root.id][\"top\"] = root.top\n",
    "    json_output[root.id][\"title\"] = root.title\n",
    "    json_output[root.id][\"text\"] = root.text\n",
    "    json_output[root.id][\"children\"] = [child.id for child in root.children]\n",
    "\n",
    "    for child in root.children:\n",
    "        get_json_output(json_output, child)\n",
    "\n",
    "\n",
    "def get_text_from_page(page, bottom, up):\n",
    "    def visitor_body_with_range(bottom, up):\n",
    "        def visitor_body(text, cm, tm, fontDict, fontSize):\n",
    "            y = tm[5]\n",
    "            if bottom < y < up:\n",
    "                parts.append(text)\n",
    "        return visitor_body\n",
    "\n",
    "    parts = []\n",
    "    page.extract_text(visitor_text=visitor_body_with_range(bottom=bottom, up=up))\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "\n",
    "def get_all_nodes(all_nodes, root):\n",
    "    all_nodes.append(root)\n",
    "    for child in root.children:\n",
    "        get_all_nodes(all_nodes, child)\n",
    "\n",
    "\n",
    "def get_page_number(pdf_reader, node):\n",
    "    if type(node.page) == PyPDF2.PageObject:\n",
    "        return pdf_reader.get_page_number(node.page)\n",
    "    return pdf_reader.get_page_number(pdf_reader.get_object(node.page))\n",
    "\n",
    "\n",
    "def adjust_page_numbers(pdf_reader, all_nodes):\n",
    "    for node in all_nodes:\n",
    "        node.page = get_page_number(pdf_reader, node)\n",
    "\n",
    "\n",
    "def float2string(all_nodes):\n",
    "    for node in all_nodes:\n",
    "        node.page = str(node.page)\n",
    "        node.top = str(node.top)\n",
    "        node.depth = str(node.depth)\n",
    "\n",
    "\n",
    "def get_text_all_nodes(pdf_reader, all_nodes):\n",
    "    prev_i = 1\n",
    "    next_i = 2\n",
    "\n",
    "    while next_i < len(all_nodes):\n",
    "\n",
    "        prev_node = all_nodes[prev_i]\n",
    "        next_node = all_nodes[next_i]\n",
    "        page = pdf_reader.pages[prev_node.page]\n",
    "\n",
    "        if prev_node.page == next_node.page:\n",
    "            text = get_text_from_page(page, bottom=next_node.top, up=prev_node.top).strip()\n",
    "            prev_node.text = text[text.find('\\n')+1:].replace('\\n', ' ')\n",
    "\n",
    "        else:\n",
    "            text = get_text_from_page(page, bottom=42, up=prev_node.top).strip()\n",
    "            text = text[text.find('\\n')+1:].replace('\\n', ' ')\n",
    "\n",
    "            curr_page_num = prev_node.page + 1\n",
    "\n",
    "            while curr_page_num < next_node.page:\n",
    "                text += pdf_reader.pages[curr_page_num].extract_text().replace('\\n', ' ')\n",
    "                curr_page_num += 1\n",
    "\n",
    "            text += get_text_from_page(pdf_reader.pages[curr_page_num], bottom=next_node.top, up=750).replace('\\n', ' ')\n",
    "            prev_node.text = text\n",
    "\n",
    "        next_i, prev_i = next_i+1, prev_i+1\n",
    "\n",
    "\n",
    "    # For the last node\n",
    "    prev_node = all_nodes[prev_i]\n",
    "    page = pdf_reader.pages[prev_node.page]\n",
    "    text = get_text_from_page(page, bottom=50, up=prev_node.top).strip()\n",
    "    text = text[text.find('\\n')+1:].replace('\\n', ' ')\n",
    "\n",
    "    curr_page_num = prev_node.page + 1\n",
    "    while curr_page_num < len(pdf_reader.pages):\n",
    "        text += pdf_reader.pages[curr_page_num].extract_text().replace('\\n', ' ')\n",
    "        curr_page_num += 1\n",
    "\n",
    "    prev_node.text = text[:text.find(\"References\")]\n",
    "\n",
    "\n",
    "def get_title_root(pdf_reader):\n",
    "    prompt = f\"What is the title of this article?\\n{pdf_reader.pages[0].extract_text()}\\nTitle: \"\n",
    "    response, _ = ask_chatgpt(prompt, history=[], system=None, new_chat=True, max_tokens=50, temp=0)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def get_depth(root, depth):\n",
    "    root.depth = depth\n",
    "    for child in root.children:\n",
    "        get_depth(child, depth+1)\n",
    "\n",
    "\n",
    "# def main\n",
    "pdf_reader = PyPDF2.PdfReader(open(pdf_file, 'rb'))\n",
    "root = create_graph(pdf_reader, title=get_title_root(pdf_reader), page=pdf_reader.pages[0])\n",
    "all_nodes = []\n",
    "json_output = {}\n",
    "get_all_nodes(all_nodes, root)\n",
    "get_depth(root, 0)\n",
    "adjust_page_numbers(pdf_reader, all_nodes)\n",
    "get_text_all_nodes(pdf_reader, all_nodes)\n",
    "# float2string(all_nodes)\n",
    "get_json_output(json_output, root)\n",
    "json_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T06:08:54.686026200Z",
     "start_time": "2023-10-15T06:08:50.984535Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# json_output2 = {}\n",
    "# for node in json_output:\n",
    "#     key = list(node.keys())[0]\n",
    "#     json_output2[key] = node[key]\n",
    "# json_output2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To Debug\n",
    "for child in root.children:\n",
    "    print(child.text, '\\n\\n')\n",
    "    # for subchild in child.children:\n",
    "    #     for subsubchild in subchild.children:\n",
    "    #         print(subsubchild.text, '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def pdf2text(pdf_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        pdf_text = ''\n",
    "        for page_number in range(len(pdf.pages)):\n",
    "            page = pdf.pages[page_number]\n",
    "            pdf_text += page.extract_text(x_tolerance=2, y_tolerance=5, layout=False).strip()\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "def count_tokens(history: list):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = 0\n",
    "    for message in history:\n",
    "        num_tokens += 4\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += -1\n",
    "    num_tokens += 2\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def count_tokens_text(text: str):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:19:14.262490500Z",
     "start_time": "2023-10-15T05:19:14.255963800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 1\n",
      "Title: Results\n",
      "Page: 2\n",
      "Text: In this section, we present several benchmarks and evaluations to demonstrate the different use cases of Cell2Sentence. All presented models are trained on a human immune tissue dataset [39] (see Section 3). \"NL + C2S\" models are pretrained on natural language and then fine-tuned on cell sentences. \"C2S\" models are only trained on cell sentences. 3Figure 2: Detailed overview of the Cell2Sentence framework. Single-cell gene expression profiles are transformed into cell sentences via expression rank orderering of gene names. Cell sentences may be annotated with biological metadata, such as cell type, tissue, or disease. LLMs are then fine-tuned on the cell sentences. Inference is done by generating cells via autoregressive cell completion, generating cells from text, or generating text from cells. The resulting generated cell sentences can be converted back to gene expression.\n",
      "Children: ['Cell sentence encoding is a robust and reversible operation', 'LLMs can meaningfully manipulate cells as text']\n",
      " \n",
      " 180\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(all_nodes[i], '\\n', count_tokens_text(all_nodes[i].text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T06:58:08.588635200Z",
     "start_time": "2023-10-15T06:58:08.572588700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The article compares self-attention layers to recurrent and convolutional layers for mapping sequences of symbol representations.\n",
      "- Three desiderata are considered: computational complexity per layer, parallelizability, and path length between long-range dependencies.\n",
      "- Self-attention layers have a constant number of sequentially executed operations and are faster than recurrent layers when the sequence length is smaller than the representation dimensionality.\n",
      "- Self-attention can be restricted to a neighborhood of size r to improve computational performance for very long sequences.\n",
      "- Convolutional layers require a stack of layers to connect all pairs of input and output positions, increasing the length of the longest paths in the network.\n",
      "- Separable convolutions decrease complexity, but self-attention combined with a point-wise feed-forward layer has similar complexity.\n",
      "- Self-attention could yield more interpretable models, as attention heads learn to perform different tasks related to the syntactic and semantic structure of sentences. \n",
      "\n",
      " 184\n"
     ]
    }
   ],
   "source": [
    "system = f'Act as a professional scientist that reviews articles.'\n",
    "article = f'article: {all_nodes[i]}'\n",
    "prompt = f'{article}.\\nExplain all important information, terms, and ideas in summarized bullet points: '\n",
    "\n",
    "response, history = ask_chatgpt(prompt, history=[], system=system, new_chat=True, max_tokens=280, temp=0)\n",
    "print(response, '\\n\\n', count_tokens_text(response))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:40:59.860403400Z",
     "start_time": "2023-10-15T05:40:56.509138100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The article discusses the use of Cell2Sentence, a framework for analyzing human immune tissue data.\n",
      "- The models used in the study are trained on a dataset of human immune tissue.\n",
      "- Two types of models are presented: \"NL + C2S\" models, which are pretrained on natural language and then fine-tuned on cell sentences, and \"C2S\" models, which are trained only on cell sentences.\n",
      "- The Cell2Sentence framework involves transforming single-cell gene expression profiles into cell sentences using expression rank ordering of gene names.\n",
      "- Cell sentences can be annotated with biological metadata such as cell type, tissue, or disease.\n",
      "- Language models (LLMs) are then fine-tuned on the cell sentences.\n",
      "- Inference can be done by generating cells via autoregressive cell completion, generating cells from text, or generating text from cells.\n",
      "- The resulting generated cell sentences can be converted back to gene expression.\n"
     ]
    }
   ],
   "source": [
    "def summarize(section, max_tokens=280, temp=0):\n",
    "\n",
    "    system = f'Act as a professional scientist that reviews articles.'\n",
    "    article = f'article: {section}'\n",
    "    prompt = f'{article}.\\nExplain all important information, terms, and ideas in summarized bullet points: '\n",
    "\n",
    "    history = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model='gpt-3.5-turbo',\n",
    "      messages=history,\n",
    "      temperature=temp,\n",
    "      max_tokens=max_tokens,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def answer_question(question, max_tokens=150, temp=0):\n",
    "\n",
    "    system = f'Act as a professional scientist that reviews articles.'\n",
    "    article = f'article: {section}'\n",
    "    prompt = f'{article}.\\nExplain all important information, terms, and ideas in summarized bullet points: '\n",
    "\n",
    "    history = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model='gpt-3.5-turbo',\n",
    "      messages=history,\n",
    "      temperature=temp,\n",
    "      max_tokens=max_tokens,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "print(summarize(all_nodes[i].text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T07:00:12.320478700Z",
     "start_time": "2023-10-15T07:00:07.537924300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = pdf2text(pdf_file)\n",
    "print(text, '\\n\\n', count_tokens_text(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "system = f'Act as a professional scientist that reviews article.'\n",
    "article = f'article: {text}'\n",
    "\n",
    "prompt = f'{article}.\\nObjective: List sections and subsections of the article. To find those sections, include several exact words of the article that followed each section and subsection.'\n",
    "\n",
    "response, history = ask_chatgpt(prompt, history=[], system=system, new_chat=True, max_tokens=1000, temp=0, model='gpt-3.5-turbo-16k')\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.find('Scaled Dot-Product Attention ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "section2text = {}\n",
    "sections_list = response.strip().split('\\n')\n",
    "section_indexes = []\n",
    "\n",
    "for section in sections_list:\n",
    "    section_indexes.append(text.find(section))\n",
    "\n",
    "print(section_indexes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text[20000:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text[2859:4780]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
